[
  {
    "prId" : 13,
    "comments" : [
      {
        "author" : {
          "login" : "pkukielka",
          "name" : null,
          "avatarUrl" : null,
          "url" : null
        },
        "body" : "We should have a way to resume crawling from last existing PR to the end if not scope is provided",
        "createdAt" : "2020-03-10T10:13:43Z",
        "updatedAt" : null,
        "commit" : "aa6e686a8328839854467529b25b40ec71f21c0b",
        "line" : null,
        "diffHunk" : "@@ -0,0 +1,39 @@\n+package github\n+\n+import java.nio.file.Path\n+import com.typesafe.config.ConfigFactory\n+\n+import github.web.{Crawler, Credentials, Repository, SearchScope}\n+import github.utils._\n+import standard.model.File\n+import standard.utils.FilesOperations\n+\n+case class CrawlToStandardModel(scope: SearchScope, credentials: Credentials, repository: Repository) {\n+  def download(): List[File] = {\n+    val crawler = Crawler(scope, credentials, repository)\n+    val standardModelFiles = crawler.downloadPullRequests().\n+      flatMap(downloadedPR => ConverterToStandardModel.convert(downloadedPR._2.toString()))\n+    standard.utils.mergeFiles(standardModelFiles)\n+  }\n+}\n+\n+object CrawlToStandardModel {\n+\n+  def main(args: Array[String]): Unit = {\n+    if(args.length > 2) {\n+      val credentials = Credentials(args(0))\n+      val searchScope = SearchScope(args(1).toInt, args(2).toInt)"
      },
      {
        "author" : {
          "login" : "pkukielka",
          "name" : null,
          "avatarUrl" : null,
          "url" : null
        },
        "body" : "Also, we could use some simple library for parsing arguments. This way we will get things like usage/help message for free. But it's not a priority.",
        "createdAt" : "2020-03-10T10:25:21Z",
        "updatedAt" : null,
        "commit" : "aa6e686a8328839854467529b25b40ec71f21c0b",
        "line" : null,
        "diffHunk" : "@@ -0,0 +1,39 @@\n+package github\n+\n+import java.nio.file.Path\n+import com.typesafe.config.ConfigFactory\n+\n+import github.web.{Crawler, Credentials, Repository, SearchScope}\n+import github.utils._\n+import standard.model.File\n+import standard.utils.FilesOperations\n+\n+case class CrawlToStandardModel(scope: SearchScope, credentials: Credentials, repository: Repository) {\n+  def download(): List[File] = {\n+    val crawler = Crawler(scope, credentials, repository)\n+    val standardModelFiles = crawler.downloadPullRequests().\n+      flatMap(downloadedPR => ConverterToStandardModel.convert(downloadedPR._2.toString()))\n+    standard.utils.mergeFiles(standardModelFiles)\n+  }\n+}\n+\n+object CrawlToStandardModel {\n+\n+  def main(args: Array[String]): Unit = {\n+    if(args.length > 2) {\n+      val credentials = Credentials(args(0))\n+      val searchScope = SearchScope(args(1).toInt, args(2).toInt)"
      }
    ]
  },
  {
    "prId" : 13,
    "comments" : [
      {
        "author" : {
          "login" : "pkukielka",
          "name" : null,
          "avatarUrl" : null,
          "url" : null
        },
        "body" : "Generally I would avoid verbs in class names (they are fine for merthods).\r\nMaybe rather `StandardModelCrawler` ro something along those lines? \r\n",
        "createdAt" : "2020-03-10T10:24:16Z",
        "updatedAt" : null,
        "commit" : "aa6e686a8328839854467529b25b40ec71f21c0b",
        "line" : null,
        "diffHunk" : "@@ -0,0 +1,39 @@\n+package github\n+\n+import java.nio.file.Path\n+import com.typesafe.config.ConfigFactory\n+\n+import github.web.{Crawler, Credentials, Repository, SearchScope}\n+import github.utils._\n+import standard.model.File\n+import standard.utils.FilesOperations\n+\n+case class CrawlToStandardModel(scope: SearchScope, credentials: Credentials, repository: Repository) {"
      }
    ]
  }
]